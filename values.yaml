---
# Default values for FADI.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

spark:
  enabled: true
  image:
    repository: dkamotsky/spark
    tag: 3.0.1-3
  worker:
    replicaCount: 3 #2 workers and 1 thriftserver (dkamotsky/spark image will assign the worker with hostname==kairos-spark-worker-0 to be thriftserver)
    autoscaling:
      enabled: false
      CpuTargetPercentage: 50
      replicasMax: 3

superset:
  enabled: true
  image:
    repository: "dkamotsky/superset"
    tag: 0.37.2-1
  persistence:
    enabled: false
  service:
    type: NodePort
  configFile: |-
    from flask_appbuilder.security.manager import AUTH_DB,AUTH_LDAP
    #---------------------------------------------------------
    # Superset specific config
    #---------------------------------------------------------
    ROW_LIMIT = 5000
    SUPERSET_WORKERS = 2
    SUPERSET_WEBSERVER_PORT = 8088
    #---------------------------------------------------------
    #---------------------------------------------------------
    # Flask App Builder configuration
    #---------------------------------------------------------
    # Your App secret key
    SECRET_KEY = '\2\1thisismyscretkey\1\2\e\y\y\h'
    # The SQLAlchemy connection string to your database backend
    # This connection defines the path to the database that stores your
    # superset metadata (slices, connections, tables, dashboards, ...).
    # Note that the connection information to connect to the datasources
    # you want to explore are managed directly in the web UI
    SQLALCHEMY_DATABASE_URI = 'sqlite:////var/lib/superset/superset.db'
    # Flask-WTF flag for CSRF
    WTF_CSRF_ENABLED = True
    # Add endpoints that need to be exempt from CSRF protection
    WTF_CSRF_EXEMPT_LIST = []
    # Set this API key to enable Mapbox visualizations
    MAPBOX_API_KEY = ''
    DEBUG=True
    LOG_FORMAT = '%(asctime)s:%(levelname)s:%(name)s:%(message)s'
    LOG_LEVEL = 'DEBUG'
    AUTH_TYPE = AUTH_LDAP
    AUTH_LDAP_SERVER = "ldap://fadi-openldap:389"
    AUTH_LDAP_USE_TLS = False
    AUTH_USER_REGISTRATION = True
    AUTH_LDAP_SEARCH = "dc=ldap,dc=cetic,dc=be"
    AUTH_LDAP_BIND_USER = "cn=admin,dc=ldap,dc=cetic,dc=be"
    AUTH_LDAP_BIND_PASSWORD = "password1"
    AUTH_LDAP_UID_FIELD = "cn"

postgresql:
  enabled: true
  persistence:
    enabled: true
  ldap:
    enabled: true
    pgldapconfig: |-
      # Reference: https://github.com/larskanis/pg-ldap-sync/blob/master/config/sample-config.yaml
      # Connection parameters to LDAP server
      ldap_connection:
        host: fadi-openldap
        port: 389
        auth:
          method: :simple
          username: CN=admin,DC=ldap,DC=cetic,DC=be
          password: password1
        # Search parameters for LDAP users which should be synchronized
      ldap_users:
        base: CN=admin,DC=ldap,DC=cetic,DC=be
        # LDAP filter (according to RFC 2254)
        # defines to users in LDAP to be synchronized
        filter: (!(cn=admin))
        # this attribute is used as PG role name
        name_attribute: cn
        # lowercase name for use as PG role name
        lowercase_name: true
      ldap_groups:
          base: DC=ldap,DC=cetic,DC=be
          filter: (|(cn=group1)(cn=group2)(cn=group3))
          # this attribute is used as PG role name
          name_attribute: cn
          # this attribute must reference to all member DN's of the given group
          member_attribute: member
      # Connection parameters to PostgreSQL server
      # see also: http://rubydoc.info/gems/pg/PG/Connection#initialize-instance_method
      pg_connection:
        host: fadi-postgresql
        dbname: postgres # the db name is usually "postgres"
        user: admin # the user name is usually "postgres"
        password: password1 # kubectl get secret --namespace fadi <pod_name> -o jsonpath="{.data.postgresql-password}" | base64 --decode
      pg_users:
        # Filter for identifying LDAP generated users in the database.
        # It's the WHERE-condition to "SELECT rolname, oid FROM pg_roles"
        # filter: rolcanlogin AND NOT rolsuper
        filter: oid IN (SELECT pam.member FROM pg_auth_members pam JOIN pg_roles pr ON pr.oid=pam.roleid WHERE pr.rolname='ldap_users')
        # Options for CREATE RULE statements
        create_options: LOGIN IN ROLE ldap_users
      pg_groups:
        # Filter for identifying LDAP generated groups in the database.
        # It's the WHERE-condition to "SELECT rolname, oid FROM pg_roles"
        # filter: NOT rolcanlogin AND NOT rolsuper
        filter: oid IN (SELECT pam.member FROM pg_auth_members pam JOIN pg_roles pr ON pr.oid=pam.roleid WHERE pr.rolname='ldap_groups')
        # Options for CREATE RULE statements
        create_options: NOLOGIN IN ROLE ldap_groups
        #grant_options:
    cron:
      schedule: "*/1 * * * *"
      repo: ceticasbl/pg-ldap-sync
      tag: latest
      restartPolicy: Never
      mountPath: /workspace
      subPath: ""
  postgresql:
    username: admin
    password: password1
    database: postgres
    pghba: |-
      local all all ldap ldapserver=fadi-openldap  ldapport=389 ldaptls=0 ldapbasedn="dc=ldap,dc=cetic,dc=be" ldapbinddn="cn=admin,dc=ldap,dc=cetic,dc=be" ldapbindpasswd=password1  ldapsearchfilter=cn=$username
      host all all 0.0.0.0/0  ldap ldapserver=fadi-openldap  ldapport=389 ldaptls=0 ldapbasedn="dc=ldap,dc=cetic,dc=be" ldapbinddn="cn=admin,dc=ldap,dc=cetic,dc=be" ldapbindpasswd=password1  ldapsearchfilter=cn=$username
    initdbscripts: |-
      #!/bin/sh
      psql -c "create role ldap_users;" postgres admin
      psql -c "create role ldap_groups;" postgres admin

minio:
  enabled: true
  persistence:
    enabled: true
    size: 50Gi
  service:
    type: NodePort

grafana:
  enabled: true
  service:
    type: NodePort
  grafana.ini:
    paths:
      data: /var/lib/grafana/data
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    analytics:
      check_for_updates: true
    log:
      mode: console
    grafana_net:
      url: https://grafana.net
    ## LDAP Authentication can be enabled with the following values on grafana.ini
    ## NOTE: Grafana will fail to start if the value for ldap.toml is invalid
    # ----- auth -------
    auth.ldap:
      enabled: true
      allow_sign_up: true
      config_file: /etc/grafana/ldap.toml
  # Enable persistence
  persistence:
    enabled: true
  # Administrator credentials when not using an existing secret (see below)
  adminUser: admin

  ## Grafana's LDAP configuration
  ## Templated by the template in _helpers.tpl
  ## NOTE: To enable the grafana.ini must be configured with auth.ldap.enabled
  ## ref: http://docs.grafana.org/installation/configuration/#auth-ldap
  ## ref: http://docs.grafana.org/installation/ldap/#configuration
  ldap:
    enabled: true
    # `existingSecret` is a reference to an existing secret containing the ldap configuration
    # for Grafana in a key `ldap-toml`.
    existingSecret: ""
    # `config` is the content of `ldap.toml` that will be stored in the created secret
    config: |-
      verbose_logging = true
      [[servers]]
      host = "fadi-openldap"
      port = 389
      use_ssl = false
      start_tls = false
      ssl_skip_verify = false
      bind_dn = "cn=admin,DC=ldap,DC=cetic,DC=be"
      bind_password = 'password1'
      search_filter = "(|(cn=%s)(&(cn=%s)(memberOf=cn=admin,DC=ldap,DC=cetic,DC=be)))"
      search_base_dns = ["cn=admin,dc=ldap,dc=cetic,dc=be"]
      group_search_base_dns = ["ou=Groups,dc=ldap,dc=cetic,dc=be"]

      [[servers.group_mappings]]
      group_dn = "*"
      org_role = "Admin"
      ##grafana_admin = true

      #group_dn = "cn=amen,DC=ldap,DC=cetic,DC=be"
      #org_role = "Viewer"

      [servers.attributes]
      name = "givenName"
      surname = "sn"
      username = "cn"
      member_of = "memberOf"
      email =  "email"

jupyterhub:
  enabled: true
  proxy:
    secretToken: 'af83775ec3bfaf0507ce596df51d491e7ed54450adc454038fa7405495465f19'
  hub:
    db:
      type: sqlite-memory
  rbac:
    enabled: true
  singleuser:
    storage:
      capacity: 5Gi
    # Defines the default image
    image:
      name: jupyter/minimal-notebook
      tag: 7d427e7a4dde
    profileList:
      - display_name: "Minimal environment"
        description: "To avoid too much bells and whistles: Python."
        default: true
      - display_name: "Datascience environment"
        description: "If you want the additional bells and whistles: Python, R, and Julia."
        kubespawner_override:
          image: jupyter/datascience-notebook:7d427e7a4dde
      - display_name: "Spark environment"
        description: "The Jupyter Stacks spark image"
        kubespawner_override:
          image: jupyter/all-spark-notebook:latest
      - display_name: "tensorflow environment"
        description: "TensorFlow Notebook"
        kubespawner_override:
          image: jupyter/tensorflow-notebook:latest
  # ---- auth ----
  auth:
    type: ldap
    ldap:
      server:
        address: fadi-openldap
      dn:
        templates:
          - 'cn={username},cn=admin,dc=ldap,dc=cetic,dc=be'
          - 'uid={username},cn=admin,dc=ldap,dc=cetic,dc=be'
          - 'cn={username},dc=ldap,dc=cetic,dc=be'
  # ---- auth ----
  prePuller:
    hook:
      enabled: false

nifi:
  enabled: true
  service:
    type: NodePort
  postStart: /opt/nifi/psql; wget -P /opt/nifi/psql https://jdbc.postgresql.org/download/postgresql-42.2.6.jar
  persistence:
    enabled: true

openldap:
  enabled: true
  persistence:
    enabled: true
  env:
    LDAP_ORGANISATION: "Cetic"
    LDAP_DOMAIN: "ldap.cetic.be"
    LDAP_BACKEND: "hdb"
    LDAP_TLS: "true"
    LDAP_TLS_ENFORCE: "false"
    LDAP_REMOVE_CONFIG_AFTER_SETUP: "false"
  adminPassword: password1
  configPassword: password2
  customLdifFiles:
    1-default-users.ldif: |-
      # You can find an example ldif file.

phpldapadmin:
  enabled: true
  image:
    tag: 0.9.0
  service:
    type: NodePort
  env:
    PHPLDAPADMIN_LDAP_HOSTS: "fadi-openldap"

kafka:
  enabled: false
  replicas: 1
  affinity: {}
  external:
    enabled: false
  persistence:
    enabled: true
  zookeeper:
    replicaCount: 1
    fullnameOverride: fadi-kafka-zookeeper

cassandra:
  enabled: false
  affinity: {}
  config:
    cluster_size: 1
    seed_size: 1
    start_rpc: true

elasticsearch:
  enabled: false

kibana:
  enabled: false
  env:
    ELASTICSEARCH_HOSTS: http://{{ .Release.Name }}-elasticsearch-client:9200

nginx_ldapauth_proxy:
  enabled: false
  service:
    type: NodePort
    externalPort: 5601
  proxy:
    port: 443
    host: "fadi-kibana"
    authName: "admin"
    ldapHost: "fadi-openldap"
    ldapDN: "dc=ldap,dc=cetic,dc=be"
    ldapFilter: "objectClass=organizationalPerson"
    ldapBindDN: "cn=admin,dc=ldap,dc=cetic,dc=be"
    requires:
      - name: "admin"
        filter: "cn=admin,dc=ldap,dc=cetic,dc=be"
  secrets:
    ldapBindPassword: "password1"

logstash:
  enabled: false
  elasticsearch:
    host: fadi-elasticsearch-client

filebeat:
  enabled: false
  config:
    output.file.enabled: false
    output.logstash:
      hosts: ["fadi-logstash:5044"]
  indexTemplateLoad:
   - fadi-elasticsearch-client:9200

tsaas:
  enabled: false

swaggerui:
  enabled: false
  swaggerui :
    jsonPath : ""
    jsonUrl : https://raw.githubusercontent.com/cetic/tsimulus-saas/master/oas/api-doc/openapi.json
    server :
      url: http://api-tsimulus.fadi.minikube
      description: "TSIMULUS API"

node-red:
  enabled: false
  service:
    type: NodePort
    port: 1880
    nodePort: 31880
  persistence:
    enabled: true
    accessMode: ReadWriteOnce
    size: 5Gi

adminer:
  enabled: true
  config:
    design: "pepa-linha"
  service:
    type: NodePort

mlflow:
  enabled: false
  backend_store_uri: "postgresql://admin:password1@fadi-postgresql.fadi.svc.cluster.local:5432"

seldon-core-operator:
  enabled: false
  usageMetrics:
    enabled: false

zabbix:
  enabled: false
  postgresql:
    enabled: false
  zabbixServer:
    DB_SERVER_HOST: "fadi-postgresql"
    POSTGRES_USER: "admin"
    POSTGRES_PASSWORD: "password1"
    POSTGRES_DB: "postgres"
  zabbixagent:
    enabled: true
    ZBX_SERVER_HOST: 127.0.01
    ZBX_SERVER_PORT: 10051
    ZBX_PASSIVE_ALLOW: true
    ZBX_PASSIVESERVERS: 127.0.0.1
    ZBX_ACTIVE_ALLOW: true
  zabbixweb:
    enabled: true
    ZBX_SERVER_HOST: zabbix-server
    ZBX_SERVER_PORT: 10051
    DB_SERVER_HOST: fadi-postgresql
    DB_SERVER_PORT: 5432
    POSTGRES_USER: admin
    POSTGRES_PASSWORD: password1
    POSTGRES_DB: postgres

hue:
  enabled: false
  image:
    registry: "dkamotsky"
    tag: "4.8.0-2"
  balancer:
    enabled: false
  ingress:
    create: false
  hue:
    replicas: 1
    database:
      create: false
      persist: true
      engine: "postgresql_psycopg2"
      host: "fadi-postgresql"
      port: 5432
      name: "hue"
    ini: |
      [desktop]
      is_hue_4=true
      secret_key=hue123
      app_blacklist=filebrowser,search,hbase,security,jobbrowser,oozie
      django_debug_mode=false
      gunicorn_work_class=gevent
      enable_prometheus=true
      use_new_editor=true
      enable_download=false
      enable_sharing=false
      use_new_side_panels=true
      use_new_charts=true
      enable_gist=false
      enable_gist_preview=false
      enable_link_sharing=false

      [[django_admins]]
        [[[admin1]]]
        name=kamotskyd
        email=kamotskyd@corning.com
        [[[admin2]]]
        name=beversbm
        email=beversbm@corning.com

      [[custom]]
        banner_top_html='<div class="container-fluid text-center"><span id="infosec-header"><h4 style="background-color:#f15757">Corning Restricted - Special Control</h4></span></div>'
        login_splash_html=<img src="/static/desktop/art/CorningLogo.png" alt='Corning' title='Corning' width="100" />
        logo_svg=<image xlink:href="/static/desktop/art/KairosLogoSmall.png" x="10" y="-10" width="60" height="60"/>

      [[task_server]]
        enabled=false

      [notebook]
        show_notebooks=false

      [dashboard]
        is_enabled=false

      [beeswax]
      # Thrift version to use when communicating with HiveServer2.
      # Version 11 comes with Hive 3.0. If issues, try 7.
      thrift_version=7

airflow:
  ingress:
    enabled: false
  airflow:
    ## configs for the docker image of the web/scheduler/worker
    ##
    image:
      repository: brettbevers/spark-airflow
      tag: latest
      ## values: Always or IfNotPresent
      pullPolicy: Always
      pullSecret: ""
    ## the airflow executor type to use
    ##
    executor: CeleryExecutor
    ## the fernet key used to encrypt the connections in the database
    ##
    fernetKey: "7T512UXSSmBOkpWimFHIVb8jK6lfmSAvx4mO6Arehnc="
    ## environment variables for the web/scheduler/worker Pods (for airflow configs)
    ##
    config:
      # Security
      AIRFLOW__CORE__SECURE_MODE: "True"
      AIRFLOW__API__AUTH_BACKEND: "airflow.api.auth.backend.deny_all"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "False"
      AIRFLOW__WEBSERVER__RBAC: "False"
      # DAGS
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      ## Disable noisy "Handling signal: ttou" Gunicorn log messages
      GUNICORN_CMD_ARGS: "--log-level WARNING"
  ###################################
  # Airflow - Scheduler Configs
  ###################################
  scheduler:
    ## custom airflow connections for the airflow scheduler
    ##
    connections:
      - id: my_aws
        type: aws
        extra: |
          {
            "aws_access_key_id": "XXXXXXXXXXXXXXXXXXX",
            "aws_secret_access_key": "XXXXXXXXXXXXXXX",
            "region_name":"eu-central-1"
          }
    ## custom airflow variables for the airflow scheduler
    ##
    variables: |
      { "environment": "dev" }
    ## custom airflow pools for the airflow scheduler
    ##
    pools: |
      {
        "example": {
          "description": "This is an example pool with 2 slots.",
          "slots": 2
        }
      }
  ###################################
  # Airflow - WebUI Configs
  ###################################
  web:
    ## configs for the Service of the web Pods
    ##
    service:
      type: NodePort
  ###################################
  # Airflow - Worker Configs
  ###################################
  workers:
    ## the number of workers Pods to run
    ##
    replicas: 1
  ###################################
  # Airflow - DAGs Configs
  ###################################
  dags:
    ## install any Python `requirements.txt` at the root of `dags.path` automatically
    ##
    ## WARNING:
    ## - if set to true, and you are using `dags.git.gitSync`, you must also enable
    ## `dags.initContainer` to ensure the requirements.txt is available at Pod start
    ##
    installRequirements: true
  ###################################
  # Database - PostgreSQL Chart
  ###################################
  postgresql:
    enabled: true
  ###################################
  # Database - Redis Chart
  ###################################
  redis:
    enabled: true